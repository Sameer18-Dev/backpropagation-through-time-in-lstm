{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 1: Introduction to Long Short-Term Memory (LSTM)**\n",
        "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture designed to capture and remember long-term dependencies in sequential data. In this notebook, we will implement a basic LSTM model using TensorFlow and explore its components."
      ],
      "metadata": {
        "id": "KjPPl45S58vq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "msrqWUmb544M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 2: LSTM Cell Implementation**\n",
        "\n",
        "The LSTM cell consists of several components: input gate, forget gate, cell update, and output gate. These gates control the flow of information through the cell, allowing it to capture long-term dependencies. The equations for each gate involve trainable weights and biases."
      ],
      "metadata": {
        "id": "3xGEEaeG55ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_cell(input_data, prev_hidden_state, prev_cell_state, weights, biases):\n",
        "    \"\"\"\n",
        "    LSTM cell implementation for a single time step.\n",
        "\n",
        "    Args:\n",
        "    - input_data: Input data for the current time step.\n",
        "    - prev_hidden_state: Hidden state from the previous time step.\n",
        "    - prev_cell_state: Cell state from the previous time step.\n",
        "    - weights: Dictionary of weights for the LSTM cell.\n",
        "    - biases: Dictionary of biases for the LSTM cell.\n",
        "\n",
        "    Returns:\n",
        "    - hidden_state: Hidden state for the current time step.\n",
        "    - cell_state: Cell state for the current time step.\n",
        "    \"\"\"\n",
        "    # Concatenate input with previous hidden state\n",
        "    concat_input = tf.concat([input_data, prev_hidden_state], axis=-1)\n",
        "\n",
        "    # Input gate\n",
        "    input_gate = tf.sigmoid(tf.matmul(concat_input, weights['input_gate']) + biases['input_gate'])\n",
        "\n",
        "    # Forget gate\n",
        "    forget_gate = tf.sigmoid(tf.matmul(concat_input, weights['forget_gate']) + biases['forget_gate'])\n",
        "\n",
        "    # Update cell state\n",
        "    cell_update = tf.tanh(tf.matmul(concat_input, weights['cell_update']) + biases['cell_update'])\n",
        "    cell_state = forget_gate * prev_cell_state + input_gate * cell_update\n",
        "\n",
        "    # Output gate\n",
        "    output_gate = tf.sigmoid(tf.matmul(concat_input, weights['output_gate']) + biases['output_gate'])\n",
        "\n",
        "    # Hidden state for the current time step\n",
        "    hidden_state = output_gate * tf.tanh(cell_state)\n",
        "\n",
        "    return hidden_state, cell_state"
      ],
      "metadata": {
        "id": "xQWsUIYI6PzV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 3: Forward Pass through LSTM Network**\n",
        "\n",
        "The forward pass involves processing input sequences through the LSTM cells, updating hidden states and cell states at each time step. This process allows the network to learn and capture temporal patterns in the data."
      ],
      "metadata": {
        "id": "1sqOuBJ16Qux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_forward(inputs, initial_hidden_state, initial_cell_state, weights, biases):\n",
        "    \"\"\"\n",
        "    Forward pass through the LSTM network.\n",
        "\n",
        "    Args:\n",
        "    - inputs: List of input sequences for each time step.\n",
        "    - initial_hidden_state: Initial hidden state.\n",
        "    - initial_cell_state: Initial cell state.\n",
        "    - weights: Dictionary of weights for the LSTM cell.\n",
        "    - biases: Dictionary of biases for the LSTM cell.\n",
        "\n",
        "    Returns:\n",
        "    - hidden_states: List of hidden states for each time step.\n",
        "    - cell_states: List of cell states for each time step.\n",
        "    \"\"\"\n",
        "    hidden_states = []\n",
        "    cell_states = []\n",
        "\n",
        "    current_hidden_state = initial_hidden_state\n",
        "    current_cell_state = initial_cell_state\n",
        "\n",
        "    for input_data in inputs:\n",
        "        current_hidden_state, current_cell_state = lstm_cell(input_data, current_hidden_state,\n",
        "                                                             current_cell_state, weights, biases)\n",
        "        hidden_states.append(current_hidden_state)\n",
        "        cell_states.append(current_cell_state)\n",
        "\n",
        "    return hidden_states, cell_states"
      ],
      "metadata": {
        "id": "AScvIDYB6WAc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 4: Backward Pass and Weight Updates**\n",
        "\n",
        "The backward pass calculates the loss and updates the weights and biases using gradient descent. We use the Adam optimizer to efficiently perform these updates."
      ],
      "metadata": {
        "id": "bvouNpj_6Wn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_backward(inputs, targets, initial_hidden_state, initial_cell_state, weights, biases, learning_rate):\n",
        "    \"\"\"\n",
        "    Backward pass and weight updates through the LSTM network.\n",
        "\n",
        "    Args:\n",
        "    - inputs: List of input sequences for each time step.\n",
        "    - targets: List of target values for each time step.\n",
        "    - initial_hidden_state: Initial hidden state.\n",
        "    - initial_cell_state: Initial cell state.\n",
        "    - weights: Dictionary of weights for the LSTM cell.\n",
        "    - biases: Dictionary of biases for the LSTM cell.\n",
        "    - learning_rate: Learning rate for weight updates.\n",
        "    \"\"\"\n",
        "    optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass to get hidden states and cell states\n",
        "        hidden_states, cell_states = lstm_forward(inputs, initial_hidden_state,\n",
        "                                                  initial_cell_state, weights, biases)\n",
        "\n",
        "        # Calculate loss using the final hidden state and target values\n",
        "        loss = tf.reduce_mean(tf.square(targets - hidden_states[-1]))\n",
        "\n",
        "    # Compute gradients with respect to weights and biases\n",
        "    gradients = tape.gradient(loss, list(weights.values()) + list(biases.values()))\n",
        "\n",
        "    # Update weights and biases using the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, list(weights.values()) + list(biases.values())))"
      ],
      "metadata": {
        "id": "5I1LhIJ46Zsz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 5: Training the LSTM Model**\n",
        "\n",
        "Training the LSTM model involves iteratively performing forward and backward passes. The model learns to predict target values by adjusting its internal parameters."
      ],
      "metadata": {
        "id": "VrOttW2s6c_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the LSTM model\n",
        "def train_lstm(inputs, targets, initial_hidden_state, initial_cell_state, weights, biases, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        lstm_backward(inputs, targets, initial_hidden_state, initial_cell_state, weights, biases, learning_rate)\n",
        "\n",
        "        # Print details after each epoch\n",
        "        if epoch % 10 == 0:\n",
        "            hidden_states, _ = lstm_forward(inputs, initial_hidden_state, initial_cell_state, weights, biases)\n",
        "            current_loss = tf.reduce_mean(tf.square(targets - hidden_states[-1]))\n",
        "\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {current_loss.numpy()}\")"
      ],
      "metadata": {
        "id": "Xs5ce9ld6duK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 6: Example Usage**\n",
        "\n",
        "An example demonstrating the usage of the implemented LSTM model with randomly generated input sequences and targets."
      ],
      "metadata": {
        "id": "yghwtzqE6efi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "sequence_length = 5\n",
        "batch_size = 1  # Adjust this based on your actual batch size\n",
        "\n",
        "# Updated weights with correct dimensions\n",
        "weights = {\n",
        "    'input_gate': tf.Variable(tf.random.normal([input_size + hidden_size, hidden_size])),\n",
        "    'forget_gate': tf.Variable(tf.random.normal([input_size + hidden_size, hidden_size])),\n",
        "    'cell_update': tf.Variable(tf.random.normal([input_size + hidden_size, hidden_size])),\n",
        "    'output_gate': tf.Variable(tf.random.normal([input_size + hidden_size, hidden_size]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'input_gate': tf.Variable(tf.zeros([hidden_size])),\n",
        "    'forget_gate': tf.Variable(tf.zeros([hidden_size])),\n",
        "    'cell_update': tf.Variable(tf.zeros([hidden_size])),\n",
        "    'output_gate': tf.Variable(tf.zeros([hidden_size]))\n",
        "}\n",
        "\n",
        "# Generate random input sequences and targets\n",
        "inputs = [tf.constant(np.random.randn(batch_size, input_size), dtype=tf.float32) for _ in range(sequence_length)]\n",
        "targets = [tf.constant(np.random.randn(batch_size, output_size), dtype=tf.float32) for _ in range(sequence_length)]\n",
        "\n",
        "initial_hidden_state = tf.Variable(tf.zeros([batch_size, hidden_size]))\n",
        "initial_cell_state = tf.Variable(tf.zeros([batch_size, hidden_size]))\n",
        "\n",
        "# Train the LSTM model\n",
        "train_lstm(inputs, targets, initial_hidden_state, initial_cell_state, weights, biases, learning_rate=0.001, epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_9L2OHK6iJy",
        "outputId": "80baa0b3-db7c-4242-e14b-455f61388254"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/300, Loss: 1.1762666702270508\n",
            "Epoch 10/300, Loss: 1.1642940044403076\n",
            "Epoch 20/300, Loss: 1.1506974697113037\n",
            "Epoch 30/300, Loss: 1.1352951526641846\n",
            "Epoch 40/300, Loss: 1.1179077625274658\n",
            "Epoch 50/300, Loss: 1.09853994846344\n",
            "Epoch 60/300, Loss: 1.0772967338562012\n",
            "Epoch 70/300, Loss: 1.0546246767044067\n",
            "Epoch 80/300, Loss: 1.0313886404037476\n",
            "Epoch 90/300, Loss: 1.0087636709213257\n",
            "Epoch 100/300, Loss: 0.9880312085151672\n",
            "Epoch 110/300, Loss: 0.9702739715576172\n",
            "Epoch 120/300, Loss: 0.9561376571655273\n",
            "Epoch 130/300, Loss: 0.9457154273986816\n",
            "Epoch 140/300, Loss: 0.938938319683075\n",
            "Epoch 150/300, Loss: 0.935623288154602\n",
            "Epoch 160/300, Loss: 0.9351335763931274\n",
            "Epoch 170/300, Loss: 0.9350149035453796\n",
            "Epoch 180/300, Loss: 0.9349077939987183\n",
            "Epoch 190/300, Loss: 0.934819221496582\n",
            "Epoch 200/300, Loss: 0.9347406625747681\n",
            "Epoch 210/300, Loss: 0.9346707463264465\n",
            "Epoch 220/300, Loss: 0.9346044659614563\n",
            "Epoch 230/300, Loss: 0.9345372319221497\n",
            "Epoch 240/300, Loss: 0.9344624280929565\n",
            "Epoch 250/300, Loss: 0.9343799352645874\n",
            "Epoch 260/300, Loss: 0.9342872500419617\n",
            "Epoch 270/300, Loss: 0.9341834783554077\n",
            "Epoch 280/300, Loss: 0.9340621829032898\n",
            "Epoch 290/300, Loss: 0.9339120984077454\n"
          ]
        }
      ]
    }
  ]
}